---
title: Getting started
sidebar_position: 1
hide_title: true
---

<h1>
  {frontMatter.title} <TierLabel tiers="enterprise" />
</h1>

# Getting Started with CAPI
## Creating your first CAPD Cluster

If you've followed the [Upgrade guide](../enterprise/upgrading.mdx) you should have a management cluster ready to roll. Next up we'll

### 1. Configure a capi provider

1. Configure `kubectl` to use the management cluster.

```bash
export KUBECONFIG=/path/to/kubeconfig
```

2. Deploy Docker's CAPD components by following the steps at https://cluster-api-aws.sigs.k8s.io/getting-started.html#install-clusterctl.

> **_NOTE:_** if you are using docker with a `kind` cluster you'll need to mount the docker socket as described in the [Install and/or configure a kubernetes cluster](https://cluster-api-aws.sigs.k8s.io/getting-started.html#install-andor-configure-a-kubernetes-cluster) kind section.

3. Enable an expermintal feature (i.e. ClusterResourceSet (CRS)).

```bash
ENABLE_CRS_FLAG_WHATEVER_IT_IS=1 clusterctl init --enable-the-docker-provider-somehow
```

See [Cluster API Providers](cluster-api-providers.mdx) page for more details on other providers.

### 2. Add a template

Save the following example to `.weave-gitops/apps/capi/templates/template.yaml`

```yaml title="template.yaml"
apiVersion: capi.weave.works/v1alpha1
kind: CAPITemplate
metadata:
  name: cluster-template-development
  namespace: default
spec:
  description: This is the std. CAPD template
  params:
    - name: CLUSTER_NAME
      description: This is used for the cluster naming.
    - name: NAMESPACE
      description: Namespace to create the cluster in.
    - name: KUBERNETES_VERSION
      description: The version of Kubernetes to use.
      options: ["1.19.11", "1.20.7"]
  resourcetemplates:
    - apiVersion: cluster.x-k8s.io/v1beta1
      kind: Cluster
      metadata:
        name: "${CLUSTER_NAME}"
        namespace: "${NAMESPACE}"
      spec:
        clusterNetwork:
          services:
            cidrBlocks:
              - 10.128.0.0/12
          pods:
            cidrBlocks:
              - 192.168.0.0/16
          serviceDomain: cluster.local
        infrastructureRef:
          apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
          kind: DockerCluster
          name: "${CLUSTER_NAME}"
          namespace: "${NAMESPACE}"
        controlPlaneRef:
          kind: KubeadmControlPlane
          apiVersion: controlplane.cluster.x-k8s.io/v1beta1
          name: "${CLUSTER_NAME}-control-plane"
          namespace: "${NAMESPACE}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerCluster
      metadata:
        name: "${CLUSTER_NAME}"
        namespace: "${NAMESPACE}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerMachineTemplate
      metadata:
        name: "${CLUSTER_NAME}-control-plane"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec:
            extraMounts:
              - containerPath: "/var/run/docker.sock"
                hostPath: "/var/run/docker.sock"
    - kind: KubeadmControlPlane
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      metadata:
        name: "${CLUSTER_NAME}-control-plane"
        namespace: "${NAMESPACE}"
      spec:
        replicas: 1
        machineTemplate:
          infrastructureRef:
            kind: DockerMachineTemplate
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            name: "${CLUSTER_NAME}-control-plane"
            namespace: "${NAMESPACE}"
        kubeadmConfigSpec:
          clusterConfiguration:
            controllerManager:
              extraArgs: { enable-hostpath-provisioner: "true" }
            apiServer:
              certSANs: [localhost, 127.0.0.1]
          initConfiguration:
            nodeRegistration:
              criSocket: /var/run/containerd/containerd.sock
              kubeletExtraArgs:
                # We have to pin the cgroupDriver to cgroupfs as kubeadm >=1.21 defaults to systemd
                # kind will implement systemd support in: https://github.com/kubernetes-sigs/kind/issues/1726
                cgroup-driver: cgroupfs
                eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
          joinConfiguration:
            nodeRegistration:
              criSocket: /var/run/containerd/containerd.sock
              kubeletExtraArgs:
                # We have to pin the cgroupDriver to cgroupfs as kubeadm >=1.21 defaults to systemd
                # kind will implement systemd support in: https://github.com/kubernetes-sigs/kind/issues/1726
                cgroup-driver: cgroupfs
                eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
        version: "${KUBERNETES_VERSION}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerMachineTemplate
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec: {}
    - apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
      kind: KubeadmConfigTemplate
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec:
            joinConfiguration:
              nodeRegistration:
                kubeletExtraArgs:
                  # We have to pin the cgroupDriver to cgroupfs as kubeadm >=1.21 defaults to systemd
                  # kind will implement systemd support in: https://github.com/kubernetes-sigs/kind/issues/1726
                  cgroup-driver: cgroupfs
                  eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
    - apiVersion: cluster.x-k8s.io/v1beta1
      kind: MachineDeployment
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        clusterName: "${CLUSTER_NAME}"
        replicas: 1
        selector:
          matchLabels:
        template:
          spec:
            clusterName: "${CLUSTER_NAME}"
            version: "${KUBERNETES_VERSION}"
            bootstrap:
              configRef:
                name: "${CLUSTER_NAME}-md-0"
                namespace: "${NAMESPACE}"
                apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
                kind: KubeadmConfigTemplate
            infrastructureRef:
              name: "${CLUSTER_NAME}-md-0"
              namespace: "${NAMESPACE}"
              apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
              kind: DockerMachineTemplate
```

See [CAPI Templates](templates.mdx) page for more details.

### 3. Add a helmrepo and annotate some helm charts

Save the following helm repo to `.weave-gitops/apps/capi/profiles/repo.yaml`:

```yaml title="repo.yaml"
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  creationTimestamp: null
  name: weaveworks-charts
  namespace: wego-system
spec:
  interval: 1m
  url: https://foot.github.io/podinfo
status: {}
```

### 4. Add a CRS to install a CNI

Create a calico configmap and a CRS as follows:

```yaml title="cni-crs.yaml"
apiVersion: addons.cluster.x-k8s.io/v1alpha3
kind: ClusterResourceSet
metadata:
  name: calico-crs
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      cni: calico
  resources:
  - kind: ConfigMap
    name: calico-crs-configmap
```

And save it to `apps/capi/bootstrap/cni-crs.yaml`

### 5. Add a cluster bootstrap config

Create a cluster bootstrap config as follows:

```yaml title="clusterbootstrapconfig.yaml"
apiVersion: capi.gitops.solutions/v1alpha1
kind: ClusterBootstrapConfig
metadata:
  name: clusterbootstrapconfig-sample
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      weave.works/spike: upa
  jobTemplates:
    - name: 'gitopsify-leaf'
      spec:
        containers:
        - image: ghcr.io/palemtnrider/wego-app:v0.5.0-rc2
          name: gitops-install
          resources: {}
          volumeMounts:
          - name: kubeconfig
            mountPath: "/etc/gitops"
            readOnly: true
          args: ["install", "--override-in-cluster", "--app-config-url", "$(GITOPS_REPO)"]
          env: 
          - name: KUBECONFIG
            value: "/etc/gitops/value"
          - name: GITOPS_REPO
            value: "ssh://git@github.com/palemtnrider/kb3.git"
          - name: GITHUB_TOKEN
            valueFrom: 
              secretKeyRef:
                name: my-pat  
                key: GITHUB_TOKEN        
          # - name: GITLAB_TOKEN # If your GitOps repo is on GitLab, use this instead of GITHUB_TOKEN
          #   valueFrom: 
          #     secretKeyRef:
          #       name: pat
          #       key: GITLAB_TOKEN        
        restartPolicy: Never
        volumes:
        - name: kubeconfig
          secret:
            secretName: '{{ .ObjectMeta.Name }}-kubeconfig'
```

And save it to `apps/capi/bootstrap/clusterbootstrapconfig.yaml`