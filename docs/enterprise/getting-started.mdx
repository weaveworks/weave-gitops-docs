---
title: Getting Started
sidebar_position: 1
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Getting Started with CAPI

### 1. Configure a capi provider

1. Configure `kubectl` to use the management cluster.

```bash
export KUBECONFIG=/path/to/kubeconfig
```

2. Deploy Docker's CAPD components by following the steps at https://cluster-api-aws.sigs.k8s.io/getting-started.html#install-clusterctl.

> **_NOTE:_** if you are using docker with a `kind` cluster you'll need to mount the docker socket as described in the [Install and/or configure a kubernetes cluster](https://cluster-api-aws.sigs.k8s.io/getting-started.html#install-andor-configure-a-kubernetes-cluster) kind section.

3. Enable an expermintal feature (i.e. ClusterResourceSet (CRS)).

```bash
ENABLE_CRS_FLAG_WHATEVER_IT_IS=1 clusterctl init --enable-the-docker-provider-somehow
```

See [Cluster API Providers](../cluster-management/cluster-api-providers.mdx) page for more details on other providers.

### 2. Add a template

Save the following example to `.weave-gitops/apps/capi/templates/template.yaml`

```yaml title="template.yaml"
apiVersion: capi.weave.works/v1alpha1
kind: CAPITemplate
metadata:
  name: cluster-template-development
spec:
  description: This is the std. CAPD template
  params:
    - name: CLUSTER_NAME
      description: This is used for the cluster naming.
    - name: NAMESPACE
      description: Namespace to create the cluster in.
    - name: KUBERNETES_VERSION
      description: The version of Kubernetes to use.
      options: ["1.19.7", "1.19.8"]
  resourcetemplates:
    - apiVersion: cluster.x-k8s.io/v1alpha3
      kind: Cluster
      metadata:
        name: "${CLUSTER_NAME}"
        namespace: "${NAMESPACE}"
      spec:
        clusterNetwork:
          pods:
            cidrBlocks:
              - 192.168.0.0/16
          serviceDomain: cluster.local
          services:
            cidrBlocks:
              - 10.128.0.0/12
        infrastructureRef:
          apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
          kind: DockerCluster
          name: "${CLUSTER_NAME}"
          namespace: "${NAMESPACE}"
        controlPlaneRef:
          kind: KubeadmControlPlane
          apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
          name: "${CLUSTER_NAME}-control-plane"
          namespace: "${NAMESPACE}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: DockerCluster
      metadata:
        name: "${CLUSTER_NAME}"
        namespace: "${NAMESPACE}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: DockerMachineTemplate
      metadata:
        name: "${CLUSTER_NAME}-control-plane"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec:
            extraMounts:
              - containerPath: "/var/run/docker.sock"
                hostPath: "/var/run/docker.sock"
    - kind: KubeadmControlPlane
      apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
      metadata:
        name: "${CLUSTER_NAME}-control-plane"
        namespace: "${NAMESPACE}"
      spec:
        replicas: 1
        infrastructureTemplate:
          kind: DockerMachineTemplate
          apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
          name: "${CLUSTER_NAME}-control-plane"
          namespace: "${NAMESPACE}"
        kubeadmConfigSpec:
          clusterConfiguration:
            controllerManager:
              extraArgs: { enable-hostpath-provisioner: "true" }
            apiServer:
              certSANs: [localhost, 127.0.0.1]
          initConfiguration:
            nodeRegistration:
              criSocket: /var/run/containerd/containerd.sock
              kubeletExtraArgs:
                eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
          joinConfiguration:
            nodeRegistration:
              criSocket: /var/run/containerd/containerd.sock
              kubeletExtraArgs:
                eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
        version: "${KUBERNETES_VERSION}"
    - apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: DockerMachineTemplate
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec: {}
    - apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
      kind: KubeadmConfigTemplate
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        template:
          spec:
            joinConfiguration:
              nodeRegistration:
                kubeletExtraArgs:
                  eviction-hard: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
    - apiVersion: cluster.x-k8s.io/v1alpha3
      kind: MachineDeployment
      metadata:
        name: "${CLUSTER_NAME}-md-0"
        namespace: "${NAMESPACE}"
      spec:
        clusterName: "${CLUSTER_NAME}"
        replicas: 1
        selector:
          matchLabels:
        template:
          spec:
            clusterName: "${CLUSTER_NAME}"
            version: "${KUBERNETES_VERSION}"
            bootstrap:
              configRef:
                name: "${CLUSTER_NAME}-md-0"
                namespace: "${NAMESPACE}"
                apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
                kind: KubeadmConfigTemplate
            infrastructureRef:
              name: "${CLUSTER_NAME}-md-0"
              namespace: "${NAMESPACE}"
              apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
              kind: DockerMachineTemplate
```

See [CAPI Templates](../cluster-management/templates.mdx) page for more details.

### 3. Add a helmrepo and annotate some helm charts

Save the following helm repo to `.weave-gitops/apps/capi/profiles/repo.yaml`:

```yaml title="repo.yaml"
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  creationTimestamp: null
  name: weaveworks-charts
  namespace: wego-system
spec:
  interval: 1m
  url: https://foot.github.io/podinfo
status: {}
```

### 4. Add a CRS to install a CNI

Create a calico configmap and a CRS as follows:

```yaml title="cni-crs.yaml"
apiVersion: addons.cluster.x-k8s.io/v1alpha3
kind: ClusterResourceSet
metadata:
  name: calico-crs
  namespace: default
spec:
  clusterSelector:
    matchLabels:
      cni: calico
  resources:
  - kind: ConfigMap
    name: calico-crs-configmap
```

And save it to `apps/capi/bootstrap/cni-crs.yaml`